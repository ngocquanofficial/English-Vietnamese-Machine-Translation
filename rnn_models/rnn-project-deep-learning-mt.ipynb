{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7214685,"sourceType":"datasetVersion","datasetId":4170975}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json, os, time\nimport torch, numpy, random\nfrom tqdm import tqdm\n\nfrom datasets import load_metric\n! pip install sacrebleu\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-17T18:34:27.856696Z","iopub.execute_input":"2023-12-17T18:34:27.856980Z","iopub.status.idle":"2023-12-17T18:34:58.670589Z","shell.execute_reply.started":"2023-12-17T18:34:27.856955Z","shell.execute_reply":"2023-12-17T18:34:58.669766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numpy.random.seed(3)\ntorch.cuda.manual_seed(3)\ntorch.manual_seed(3)\nrandom.seed(3)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:34:58.672230Z","iopub.execute_input":"2023-12-17T18:34:58.672793Z","iopub.status.idle":"2023-12-17T18:34:58.745125Z","shell.execute_reply.started":"2023-12-17T18:34:58.672766Z","shell.execute_reply":"2023-12-17T18:34:58.744148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import wandb\nexcept:\n    ! pip install wandb\nimport wandb\nwandb.login(\n    key = \"280aa3837eb27ece3c32ed8e27e3e233d0afdc9c\"\n)\nwandb.init(\"RNN Encoder - Decoder Model For Project Deep Learning\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:34:58.746285Z","iopub.execute_input":"2023-12-17T18:34:58.746568Z","iopub.status.idle":"2023-12-17T18:35:32.894653Z","shell.execute_reply.started":"2023-12-17T18:34:58.746544Z","shell.execute_reply":"2023-12-17T18:35:32.893551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 0. Config","metadata":{}},{"cell_type":"code","source":"config = {\n    ### Thông số bộ dataset\n    \"train_data_file\": \"/kaggle/input/phomt-dl-2023-1/PhoMT_json/tokenization/train/train.json\",\n    \"dev_data_file\": \"/kaggle/input/phomt-dl-2023-1/PhoMT_json/tokenization/dev/dev.json\",\n    \"test_data_file\": \"/kaggle/input/phomt-dl-2023-1/PhoMT_json/tokenization/test/test.json\",\n    \"small_train_data\": 30000, # nếu chỉ train bộ data nhỏ, phải set = số data sẽ sample ra, ví dụ 100 000.\n    \n    ### Thông số train\n    \"batch_size\" : 16,\n    \"epoch\": 10,\n    \n    ### Thông số model:\n    \"initial_model\": None, # path của file model.pth để load lại model cho train tiếp hoặc infer\n    \"embedding_dim\": 256,\n    \"hidden_size\": 1024,\n    \n    ### Thông số opimizer và lr_scheduler:\n    \"step_size\": 2,\n    \"gamma\": 0.2,\n    \n    ### Output folder\n    \"model_save_path\": \"/kaggle/working/save_model.pth\"\n    \n}","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:32.897365Z","iopub.execute_input":"2023-12-17T18:35:32.897705Z","iopub.status.idle":"2023-12-17T18:35:32.906562Z","shell.execute_reply.started":"2023-12-17T18:35:32.897673Z","shell.execute_reply":"2023-12-17T18:35:32.905690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Các hàm phụ","metadata":{}},{"cell_type":"code","source":"def load_model():\n    statedict_2 = torch.load(config[\"initial_model\"])\n    encoder_2.load_state_dict(statedict_2['encoder'])\n    decoder_2.load_state_dict(statedict_2['decoder'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Phần dataset**","metadata":{}},{"cell_type":"code","source":"def preprocess(json_file, train = True):\n    en_sentences = list()\n    vi_sentences = list()\n    \n    with open(json_file, \"r\") as f:\n        data = json.load(f)[\"data\"]\n        \n    if train and config[\"small_train_data\"] != 0:\n        data = numpy.random.choice(a = data, \n                                   size = config[\"small_train_data\"])\n        \n    for sample in data:\n        en_sentences.append(sample[\"translation\"][\"en\"].strip().lower())\n        vi_sentences.append(sample[\"translation\"][\"vi\"].strip().lower())\n    return en_sentences, vi_sentences","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:32.907938Z","iopub.execute_input":"2023-12-17T18:35:32.908278Z","iopub.status.idle":"2023-12-17T18:35:32.927060Z","shell.execute_reply.started":"2023-12-17T18:35:32.908246Z","shell.execute_reply":"2023-12-17T18:35:32.926161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Lang:\n    def __init__(self, sentence_list, train=True, word2id=None, id2word=None):\n        self.word2id = word2id\n        self.id2word = id2word\n        self.train = train\n        self.preprocess(sentence_list)\n        self.get_vocab()\n        self.get_word_vectors()\n    \n    def preprocess(self, sentence_list):\n        \"\"\"\n        Preprocess các câu trong sentence_list:\n        - thêm 2 token <START> và <END>\n        - Padding các câu bằng các token <PAD>\n        \"\"\"\n        \n        ### Thêm 2 token <START> và <END>\n        self.max_len = 0\n        self.sentences = []\n        for sen in sentence_list:\n            sen = '<START> ' + sen + ' <END>'\n            length = len(sen.split())\n            self.sentences.append(sen)\n            if self.max_len < length:\n                self.max_len = length\n        \n        ### Padding\n        for i, sen in enumerate(self.sentences):\n            length = len(sen.split())\n            diff = self.max_len - length\n            paddings = [' <PAD>'] * diff\n            self.sentences[i] = sen + ''.join(paddings)\n    \n    def get_vocab(self):\n        \"\"\"\n        Tạo word2id, id2word, vocab size.\n        \"\"\"\n        if self.train:\n            self.word2id = {}\n            self.id2word = []\n            for s in self.sentences:\n                for char in s.split():\n                    if char not in self.word2id:\n                        self.id2word.append(char)\n                        self.word2id[char] = len(self.id2word) - 1\n        self.vocab_size = len(self.id2word)\n    \n    def get_word_vectors(self):\n        \"\"\"\n        Tạo word vectors.\n        \"\"\"\n        self.wordvec = []\n        for i, sen in enumerate(self.sentences):\n            id_list = []\n            for s in sen.split():\n                if s in self.word2id:\n                    id_list.append(self.word2id[s])\n                else:\n                    id_list.append(random.randint(0, self.vocab_size-1))\n            self.wordvec.append(id_list)\n        self.wordvec = numpy.array(self.wordvec)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:32.928291Z","iopub.execute_input":"2023-12-17T18:35:32.928541Z","iopub.status.idle":"2023-12-17T18:35:32.943569Z","shell.execute_reply.started":"2023-12-17T18:35:32.928518Z","shell.execute_reply":"2023-12-17T18:35:32.942693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MTDataset(Dataset):\n    def __init__(self, \n                 input_matrix,  # word vectors of input sentences\n                 target_matrix  # word vectors of output sentences\n                ):\n        self.data = []\n        for i in range(len(input_matrix)):\n            self.data.append((input_matrix[i], target_matrix[i]))\n            \n    def __getitem__(self, idx):\n        return (torch.Tensor(self.data[idx][0]), torch.Tensor(self.data[idx][1]))\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:32.944745Z","iopub.execute_input":"2023-12-17T18:35:32.945322Z","iopub.status.idle":"2023-12-17T18:35:32.956516Z","shell.execute_reply.started":"2023-12-17T18:35:32.945296Z","shell.execute_reply":"2023-12-17T18:35:32.955688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Model**","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self,\n                 en_vocab_size: int, # số lượng từ gtrong vocab ngôn ngữ input (en)\n                 embedding_dim: int, # số chiều của vector embedding\n                 hidden_size: int,   # số chiều của vectoer state h trong GRU\n                ):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        \n        # 1. Lớp Embedding\n        self.embedding_layer = nn.Embedding(num_embeddings = en_vocab_size,\n                                            embedding_dim = embedding_dim)\n        # 2. Mạng GRU\n        self.gru = nn.GRU(input_size = embedding_dim, \n                          hidden_size = hidden_size,\n                          batch_first=True, \n                          bidirectional=True)\n    \n    def forward(self, x):\n        embedding = self.embedding_layer(x)\n        output, hidden = self.gru(embedding)\n        last_backward_hidden = output[:, 0, self.hidden_size:].unsqueeze(0)\n        \n        return output, last_backward_hidden","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:32.957987Z","iopub.execute_input":"2023-12-17T18:35:32.958346Z","iopub.status.idle":"2023-12-17T18:35:32.967611Z","shell.execute_reply.started":"2023-12-17T18:35:32.958315Z","shell.execute_reply":"2023-12-17T18:35:32.966834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, hidden_size, vocab_size, embedding_dim):\n        super(Decoder, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        \n        # Alignment model\n        self.Wa = nn.Linear(self.hidden_size, self.hidden_size)\n        self.Ua = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.Va = nn.Linear(self.hidden_size, 1)\n        self.softmax = nn.Softmax(dim=1)\n        \n        # GRU layer\n        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n        self.gru = nn.GRU(self.embedding_dim + self.hidden_size * 2, self.hidden_size, batch_first=True)\n        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n        \n    def forward(self, dec_input, hidden, enc_out):\n        Tx = enc_out.shape[1]\n        hidden_repeat = hidden.permute(1, 0, 2).repeat(1, Tx, 1)\n        energies = self.Va(torch.tanh(self.Wa(hidden_repeat) + self.Ua(enc_out)))\n        alphas = self.softmax(energies)\n        context = torch.sum(alphas * enc_out, dim=1).unsqueeze(1)\n        embedding = self.embedding(dec_input.unsqueeze(1))\n        gru_input = torch.cat((embedding, context), dim=-1)\n        out, hidden = self.gru(gru_input, hidden.contiguous())\n        out = self.out(out)\n        return out, hidden","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:32.968840Z","iopub.execute_input":"2023-12-17T18:35:32.969748Z","iopub.status.idle":"2023-12-17T18:35:32.982468Z","shell.execute_reply.started":"2023-12-17T18:35:32.969716Z","shell.execute_reply":"2023-12-17T18:35:32.981656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_encoder(en_vocab_size: int):\n    encoder = Encoder(en_vocab_size = en_vocab_size,\n                      embedding_dim = config[\"embedding_dim\"],\n                      hidden_size = config[\"hidden_size\"]).to(device)\n    return encoder\n\ndef get_decoder(vi_vocab_size: int):\n    decoder = Decoder(hidden_size = config[\"hidden_size\"],\n                      vocab_size = vi_vocab_size,\n                      embedding_dim = config[\"embedding_dim\"]).to(device)\n    return decoder","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:32.985709Z","iopub.execute_input":"2023-12-17T18:35:32.985980Z","iopub.status.idle":"2023-12-17T18:35:32.995553Z","shell.execute_reply.started":"2023-12-17T18:35:32.985957Z","shell.execute_reply":"2023-12-17T18:35:32.994740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Loss function**","metadata":{}},{"cell_type":"code","source":"def getLossfn():\n    return nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:32.996861Z","iopub.execute_input":"2023-12-17T18:35:32.997206Z","iopub.status.idle":"2023-12-17T18:35:33.004825Z","shell.execute_reply.started":"2023-12-17T18:35:32.997172Z","shell.execute_reply":"2023-12-17T18:35:33.003978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Optimizer (and Learning rate Scheduler)**","metadata":{}},{"cell_type":"code","source":"def getOptimizer(encoder: nn.Module, decoder: nn.Module):\n    optimizer = torch.optim.Adam(params = list(encoder.parameters()) + list(decoder.parameters())\n                                )\n    return optimizer\n\ndef getLrScheduler(optimizer: torch.optim.Adam):\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                   step_size = config[\"step_size\"],\n                                                   gamma = config[\"gamma\"])\n    return lr_scheduler","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:33.008252Z","iopub.execute_input":"2023-12-17T18:35:33.008503Z","iopub.status.idle":"2023-12-17T18:35:33.015642Z","shell.execute_reply.started":"2023-12-17T18:35:33.008481Z","shell.execute_reply":"2023-12-17T18:35:33.014785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Hàm train và eval**","metadata":{}},{"cell_type":"code","source":"### Hàm dịch 1 câu \ndef translate(sentence, en_word2id, vi_word2id, vi_id2word, encoder, decoder, vi_max_len):\n\n    sentence = '<START> ' + sentence.strip().lower() + ' <END>'\n    sen_matrix = [en_word2id[s] for s in sentence.split()]\n    sen_tensor = torch.Tensor(sen_matrix).to(device=device, dtype=torch.long).unsqueeze(0)\n    encoder.eval()\n    decoder.eval()\n    with torch.no_grad():\n        enc_out, enc_hidden = encoder(sen_tensor)\n        dec_hidden = enc_hidden\n        dec_input = torch.Tensor([vi_word2id['<START>']]).to(device, dtype=torch.long)\n        output_list = []\n        for t in range(1, vi_max_len):\n            out, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n            dec_input = torch.max(out, dim=-1)[1].squeeze(1)\n            next_id = dec_input.squeeze().clone().cpu().numpy()\n            next_word = vi_id2word[next_id]\n            if next_word == '<END>':\n                break\n            output_list.append(next_word)\n        return ' '.join(output_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:33.016723Z","iopub.execute_input":"2023-12-17T18:35:33.017432Z","iopub.status.idle":"2023-12-17T18:35:33.028450Z","shell.execute_reply.started":"2023-12-17T18:35:33.017400Z","shell.execute_reply":"2023-12-17T18:35:33.027607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Hàm Train\ndef train(train_loader, \n          encoder, decoder, loss_fn, optimizer, lr_scheduler,\n          id2word\n         ):\n    \n    encoder.train()\n    decoder.train()\n    best_bleu = 0\n    best_statedict = {'encoder': encoder.state_dict(), 'decoder': decoder.state_dict()}\n    \n    for epoch in range(config[\"epoch\"]):\n        print(f\"Start epoch {epoch}\")\n        t1 = time.time()\n        ### Part 1. Train\n        train_loss = 0\n        for i, (x, y) in enumerate(train_loader):\n            x = x.type(torch.long).to(device)\n            y = y.type(torch.long).to(device)\n            \n            enc_out, enc_hidden = encoder(x)\n            dec_hidden = enc_hidden\n            dec_input = y[:, 0]\n            loss = 0\n            optimizer.zero_grad()\n            for t in range(1, y.size(1)):\n                out, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n                dec_input = y[:, t]\n                loss += loss_fn(out.squeeze(1), y[:, t])\n            \n            train_loss += loss\n            loss.backward()\n            optimizer.step()\n        \n        train_loss /= len(train_loader)\n        train_loss = train_loss.detach().cpu().item()\n        \n        lr_scheduler.step()\n        \n        ### Part 2. Eval\n        t2 = time.time()\n        bleu, num_oov = eval_bleu(en_sentences_dev, vi_sentences_dev, en_train.word2id, vi_train.word2id, vi_train.id2word, encoder, decoder, vi_train.max_len )\n        t3 = time.time()\n        \n        if bleu > best_bleu:\n            best_statedict = {'encoder': encoder.state_dict(), 'decoder': decoder.state_dict()}\n            best_bleu = bleu\n        \n        ### Print:\n        print(f\"Train loss: {train_loss}\")\n        print(f\"Train time: {t2-t1}\")\n        print(f\"Bleu score on dev set: {bleu} and num_oov = {num_oov}\")\n        print(f\"Eval Bleu time: {t3 - t2}\")\n        print(f\"End epoch {epoch}\\n********************************************************\")\n        \n        ### wandb\n        wandb.log({\n            \"Train loss\": train_loss,\n            \"Bleu score on dev set\": bleu\n        })\n        \n    return best_statedict, best_bleu","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:33.029901Z","iopub.execute_input":"2023-12-17T18:35:33.030437Z","iopub.status.idle":"2023-12-17T18:35:33.045541Z","shell.execute_reply.started":"2023-12-17T18:35:33.030405Z","shell.execute_reply":"2023-12-17T18:35:33.044597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Eval, tính BLEU score\ndef eval_bleu(en_sentences_test, vi_sentences_test,\n              en_word2id, vi_word2id, vi_id2word,\n              encoder, decoder, vi_max_len\n             ):\n    bleu_metric = load_metric(\"sacrebleu\")\n    number_oov = 0\n    \n    ### Dịch bộ test\n    translated_sentences = list()\n    reference_sentences = list()\n    for idx, en_sen in enumerate(en_sentences_test):\n        oov = False\n        try:\n            translate_sen = translate(en_sen, en_word2id, vi_word2id, vi_id2word, encoder, decoder, vi_max_len)\n        except: # Nếu bị out of vocab\n            number_oov += 1\n            oov = True\n        if not oov:\n            translated_sentences.append(translate_sen)\n            reference_sentences.append(vi_sentences_test[idx])\n    \n    ### Tính Bleu\n    for translation, reference in zip(translated_sentences, reference_sentences):\n        bleu_metric.add(prediction = translation, reference = [reference])\n    \n    bleu_score = bleu_metric.compute()\n    return bleu_score, num_oov","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:33.046716Z","iopub.execute_input":"2023-12-17T18:35:33.047054Z","iopub.status.idle":"2023-12-17T18:35:33.058720Z","shell.execute_reply.started":"2023-12-17T18:35:33.047024Z","shell.execute_reply":"2023-12-17T18:35:33.057888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Run**","metadata":{"execution":{"iopub.status.busy":"2023-12-17T14:00:17.967128Z","iopub.execute_input":"2023-12-17T14:00:17.967624Z","iopub.status.idle":"2023-12-17T14:00:17.975704Z","shell.execute_reply.started":"2023-12-17T14:00:17.967570Z","shell.execute_reply":"2023-12-17T14:00:17.974189Z"}}},{"cell_type":"code","source":"### 1. Tạo dataset\nen_sentences_train, vi_sentences_train = preprocess(config[\"train_data_file\"])\nen_sentences_dev, vi_sentences_dev = preprocess(config[\"dev_data_file\"], train = False)\nen_sentences_test, vi_sentences_test = preprocess(config[\"test_data_file\"], train = False)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:33.059791Z","iopub.execute_input":"2023-12-17T18:35:33.060153Z","iopub.status.idle":"2023-12-17T18:35:58.435279Z","shell.execute_reply.started":"2023-12-17T18:35:33.060122Z","shell.execute_reply":"2023-12-17T18:35:58.434169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_train, vi_train = Lang(en_sentences_train), Lang(vi_sentences_train)\nen_dev, vi_dev = Lang(en_sentences_dev, train= False, word2id = en_train.word2id, id2word = en_train.id2word), Lang(vi_sentences_dev, train= False, word2id = vi_train.word2id, id2word = vi_train.id2word)\nen_test, vi_test = Lang(en_sentences_test, train= False, word2id = en_train.word2id, id2word = en_train.id2word), Lang(vi_sentences_test, train= False, word2id = vi_train.word2id, id2word = vi_train.id2word)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:35:58.436661Z","iopub.execute_input":"2023-12-17T18:35:58.437052Z","iopub.status.idle":"2023-12-17T18:36:14.641996Z","shell.execute_reply.started":"2023-12-17T18:35:58.437013Z","shell.execute_reply":"2023-12-17T18:36:14.640926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = MTDataset(en_train.wordvec, vi_train.wordvec)\ntrain_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:36:14.643378Z","iopub.execute_input":"2023-12-17T18:36:14.643742Z","iopub.status.idle":"2023-12-17T18:36:14.679638Z","shell.execute_reply.started":"2023-12-17T18:36:14.643708Z","shell.execute_reply":"2023-12-17T18:36:14.678728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### 2. Tạo model\nencoder = get_encoder(en_train.vocab_size)\ndecoder = get_decoder(vi_train.vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:36:14.692528Z","iopub.execute_input":"2023-12-17T18:36:14.692933Z","iopub.status.idle":"2023-12-17T18:36:23.468002Z","shell.execute_reply.started":"2023-12-17T18:36:14.692899Z","shell.execute_reply":"2023-12-17T18:36:23.467020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### 3. Tạo loss\nloss_fn = getLossfn()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:36:23.469534Z","iopub.execute_input":"2023-12-17T18:36:23.469969Z","iopub.status.idle":"2023-12-17T18:36:23.475314Z","shell.execute_reply.started":"2023-12-17T18:36:23.469929Z","shell.execute_reply":"2023-12-17T18:36:23.474315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### 4. Tạo optimizer và lr_scheduler\noptimizer = getOptimizer(encoder, decoder)\nlr_scheduler = getLrScheduler(optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:36:23.476345Z","iopub.execute_input":"2023-12-17T18:36:23.476619Z","iopub.status.idle":"2023-12-17T18:36:23.485768Z","shell.execute_reply.started":"2023-12-17T18:36:23.476584Z","shell.execute_reply":"2023-12-17T18:36:23.484701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Train & eval on dev set**","metadata":{}},{"cell_type":"code","source":"### Train\nbest_statedict, best_bleu = train(train_dataloader, encoder, decoder, loss_fn, optimizer, lr_scheduler, vi_train.id2word)\nprint(\"Complete train!\")\nprint(f\"The best bleu score on dev set: {best_bleu}\")\nprint()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Eval on test set**","metadata":{}},{"cell_type":"code","source":"t1 = time.time()\nbleu, num_oov = eval_bleu(en_sentences_test, vi_sentences_test, en_train.word2id, vi_train.word2id, vi_train.id2word, encoder, decoder, vi_train.max_len )\nt2 = time.time()\nprint(f\"Time for eval bleu score on test set: {t2 - t1}\")\nprint(f\"BLEU and OOV on test set: {bleu_score, num_oov}\")\nprint()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:36:23.486924Z","iopub.execute_input":"2023-12-17T18:36:23.487161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **SAVE**","metadata":{}},{"cell_type":"code","source":"torch.save(best_statedict, config[\"model_save_path\"])\nprint(\"Saved model!\")","metadata":{},"execution_count":null,"outputs":[]}]}
